---
title: paper-304
layout: page
---
<head>
<style>
div1 {
  background-color: lightgrey;
  width: 30px;
  border: 1px solid green;
  padding: 5px;
  margin: 1px;
}
.button {
  border: none;
  color: white;
  padding: 6px 6px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  font-size: 16px;
  margin: 4px 2px;
  transition-duration: 0.4s;
  cursor: pointer;
}.button1 {
  background-color: white; 
  color: black; 
  border: 2px solid #008CBA;
}
.button1:hover {
  background-color: #008CBA;
  color: white;
}.button2 {
  background-color: white;
  color: black; 
  border: 2px solid red;
}
.button2:hover {
  background-color: red;
  color: white;
}
</style>
</head>
<div class='container'>
  <div class='row'>
      <div class='3u 12u(mobile)'>
      <section>
       <button class='button button2' onclick=" window.open('https://ojs.aaai.org/index.php/ICAPS/article/view/19854/19613','_blank')"><input type='image' src='../../images/hyperlink-logo.png' width='14' height='13'> PDF</button>
<br><br><h3 class='top'>Talk Sessions: </h3>
<button class='button button1' onclick=" window.open('http://icaps22.icaps-conference.org/schedule#16a','_blank')"><input type='image' src='../../images/hyperlink-blue.png' width='14' height='13'>June 22, Session 16a</button>
<button class='button button1' onclick=" window.open('http://icaps22.icaps-conference.org/schedule#29a','_blank')"><input type='image' src='../../images/hyperlink-blue.png' width='14' height='13'>June 24, Session 29a</button>
<br><br><h3 class='top'>Poster Sessions: </h3>
<button class='button button2' onclick=" window.open('http://icaps22.icaps-conference.org/posters/ICAPS_2022_Paper_304_Poster.pdf','_blank')"><input type='image' src='../../images/hyperlink-logo.png' width='14' height='13'> Poster</button><br><br>
<div1> June 22, Booth 42</div1> <br><br>
<div1> June 24, Booth 29</div1> <br><br>
</section>
 </div>
<div class='9u 12u(mobile)'>
    <section>
      <header>
        <h1 class='top'>Multi-Agent Tree Search with Dynamic Reward Shaping</h1>
        <h3>Alvaro Velasquez, Brett Bissey, Lior Barak, Daniel Melcer, Andre Beckus, Ismail Alkhouri and George Atia</h3>
      </header>
<b>Abstract:</b> Sparse rewards and their representation in multi-agent domains remains a challenge for the development of multi-agent planning systems. While techniques from formal methods can be adopted to represent the underlying planning objectives, their use in facilitating and accelerating learning has witnessed limited attention in multi-agent settings. Reward shaping methods that leverage such formal representations in single-agent settings are typically static in the sense that the artificial rewards remain the same throughout the entire learning process. In contrast, we investigate the use of such formal objective representations to define novel reward shaping functions that capture the learned experience of the agents. More specifically, we leverage the automaton representation of the underlying team objectives in mixed cooperative-competitive domains such that each automaton transition is assigned an expected value proportional to the frequency with which it was observed in successful trajectories of past behavior. This form of dynamic reward shaping is proposed within a multi-agent tree search architecture wherein agents can simultaneously reason about the future behavior of other agents as well as their own future behavior.<br><br>
<div id='presentation-embed-38982891'></div>
<script src='https://slideslive.com/embed_presentation.js'></script>
<script>
embed = new SlidesLiveEmbed('presentation-embed-38982891', { 
 presentationId: '38982891',
 autoPlay: false,
 verticalEnabled: true, 
   });
</script>
</section>
  </div>
  </div>
</div>